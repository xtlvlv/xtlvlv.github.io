<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/head.jpeg">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"xtlvlv.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":2,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="置顶 不知道为什么，我就是不想学机器学习。。。但为了完成作业，还是得了解一下。 之前用过sklearn，也学习过机器学习的知识，现在有点忘了，再整理一下。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习与深度学习">
<meta property="og:url" content="https://xtlvlv.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="不知自己不知">
<meta property="og:description" content="置顶 不知道为什么，我就是不想学机器学习。。。但为了完成作业，还是得了解一下。 之前用过sklearn，也学习过机器学习的知识，现在有点忘了，再整理一下。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-01-06T03:19:03.000Z">
<meta property="article:modified_time" content="2021-01-06T03:21:53.477Z">
<meta property="article:author" content="不知自己不知">
<meta property="article:tag" content="python">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://xtlvlv.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习与深度学习 | 不知自己不知</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


 <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">

<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
<!-- require MetingJS-->

<script src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script> 

  <!--pjax：防止跳转页面音乐暂停-->
 <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script>
  
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">不知自己不知</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">59</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">13</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">70</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xtlvlv.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpeg">
      <meta itemprop="name" content="不知自己不知">
      <meta itemprop="description" content="以阅读为生是一件很酷的事，以写作为生是一件更酷的事。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不知自己不知">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习与深度学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-01-06 11:19:03 / 修改时间：11:21:53" itemprop="dateCreated datePublished" datetime="2021-01-06T11:19:03+08:00">2021-01-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论数：</span>
    
    <a title="valine" href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>17k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>16 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="置顶"><a href="#置顶" class="headerlink" title="置顶"></a>置顶</h1><ol>
<li>不知道为什么，我就是不想学机器学习。。。但为了完成作业，还是得了解一下。</li>
<li>之前用过sklearn，也学习过机器学习的知识，现在有点忘了，再整理一下。<a id="more"></a>
</li>
</ol>
<hr>
<h1 id="一、机器学习概述"><a href="#一、机器学习概述" class="headerlink" title="一、机器学习概述"></a>一、机器学习概述</h1><h3 id="1-机器学习算法分类"><a href="#1-机器学习算法分类" class="headerlink" title="1. 机器学习算法分类"></a>1. 机器学习算法分类</h3><h5 id="1-监督学习"><a href="#1-监督学习" class="headerlink" title="1. 监督学习"></a>1. 监督学习</h5><ul>
<li>定义<ul>
<li>输入特征值和目标值，输出连续的值(回归)或离散值(分类)</li>
</ul>
</li>
<li>分类算法<ul>
<li>k-近邻算法，贝叶斯分类，决策树和随机森林，逻辑回归，SVM</li>
</ul>
</li>
<li>回归算法  <ul>
<li>线性回归，岭回归</li>
</ul>
</li>
</ul>
<h5 id="2-无监督学习"><a href="#2-无监督学习" class="headerlink" title="2. 无监督学习"></a>2. 无监督学习</h5><ul>
<li>定义<ul>
<li>输入特征值，无标签</li>
</ul>
</li>
<li>算法  <ul>
<li>聚类k-means</li>
</ul>
</li>
</ul>
<h3 id="2-机器学习开发流程"><a href="#2-机器学习开发流程" class="headerlink" title="2. 机器学习开发流程"></a>2. 机器学习开发流程</h3><ol>
<li>获取数据</li>
<li>数据处理(清洗)(可以用pandas)</li>
<li>特征工程</li>
<li>机器学习算法训练模型(可以用sklearn)</li>
<li>模型评估，若不好回到2-4，好的话继续</li>
<li>应用</li>
</ol>
<h3 id="3-一般的算法工程师的工作"><a href="#3-一般的算法工程师的工作" class="headerlink" title="3. 一般的算法工程师的工作"></a>3. 一般的算法工程师的工作</h3><ol>
<li>分析数据</li>
<li>分析业务</li>
<li>应用常见的算法</li>
<li>特征工程，调参，优化</li>
</ol>
<h3 id="4-框架"><a href="#4-框架" class="headerlink" title="4. 框架"></a>4. 框架</h3><ol>
<li>机器学习<ul>
<li>scikit learn(sklearn)</li>
</ul>
</li>
<li>深度学习<ul>
<li>tensorflow(最火)</li>
<li>pytorch/caffe2等</li>
</ul>
</li>
</ol>
<h3 id="5-没有免费的午餐"><a href="#5-没有免费的午餐" class="headerlink" title="5. 没有免费的午餐"></a>5. 没有免费的午餐</h3><ul>
<li>某个算法在某个问题中效果好，在另一个问题中可能效果就不好</li>
</ul>
<h1 id="二、特征工程"><a href="#二、特征工程" class="headerlink" title="二、特征工程"></a>二、特征工程</h1><h3 id="1-获取数据集"><a href="#1-获取数据集" class="headerlink" title="1. 获取数据集"></a>1. 获取数据集</h3><h3 id="2-数据集划分"><a href="#2-数据集划分" class="headerlink" title="2. 数据集划分"></a>2. 数据集划分</h3><ul>
<li>划分为训练集和测试集，比例通常为7:3左右</li>
</ul>
<h3 id="3-特征抽取"><a href="#3-特征抽取" class="headerlink" title="3. 特征抽取"></a>3. 特征抽取</h3><ul>
<li>将任何数据(如文本和图像)转换成可用于机器学习的数字特征<ul>
<li>字典特征处理(特征离散化)</li>
<li>文本特征处理</li>
<li>图像特征处理</li>
</ul>
</li>
</ul>
<h3 id="4-特征预处理"><a href="#4-特征预处理" class="headerlink" title="4. 特征预处理"></a>4. 特征预处理</h3><ul>
<li>数值型数据的无量纲化<ol>
<li>归一化<ul>
<li>即让数据归一到0~1区间</li>
<li>受异常值(特别大的值和特别小的值)影响较大</li>
<li>适用于传统精确小数据场景</li>
</ul>
</li>
<li>标准化<ul>
<li>x’=(x-mean)/a，a为标准差</li>
<li>适用于现代嘈杂大数据场景<h3 id="5-特征降维"><a href="#5-特征降维" class="headerlink" title="5. 特征降维"></a>5. 特征降维</h3></li>
</ul>
</li>
</ol>
</li>
<li>定义：降低特征个数，得到一组”不箱管”主变量的过程<ul>
<li>例如：相关特征：相对湿度和降雨量，二者数据冗余，若冗余过多，降低处理速度</li>
</ul>
</li>
</ul>
<h1 id="三、分类算法"><a href="#三、分类算法" class="headerlink" title="三、分类算法"></a>三、分类算法</h1><h3 id="1-KNN-K-近邻算法"><a href="#1-KNN-K-近邻算法" class="headerlink" title="1. KNN/K-近邻算法"></a>1. KNN/K-近邻算法</h3><ol>
<li>原理<ul>
<li>如果一个样本在特征空间中的k个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本也属于这个类别</li>
</ul>
</li>
<li>问题<ul>
<li>k值取得过小，会受异常点影响</li>
<li>k值取得过大，样本不均衡会影响</li>
</ul>
</li>
<li>优点<ul>
<li>简单易理解，易于实现，无需训练</li>
</ul>
</li>
<li>缺点<ul>
<li>k值选择不当则分类精度不能保证</li>
<li>懒惰算法，对测试样本分类时的计算量大，内存开销大</li>
</ul>
</li>
<li>使用场景<ul>
<li>小数据场景，千级别的数据</li>
</ul>
</li>
</ol>
<h3 id="2-朴素贝叶斯算法-常用与文本分类"><a href="#2-朴素贝叶斯算法-常用与文本分类" class="headerlink" title="2. 朴素贝叶斯算法(常用与文本分类)"></a>2. 朴素贝叶斯算法(常用与文本分类)</h3><ol>
<li>定义<ul>
<li>朴素：假设特征与特征之间是相互独立的</li>
<li>贝叶斯：贝叶斯公式</li>
<li>分类完后样本属于各个类别有不同的概率值，取概率值大的为预测类别</li>
</ul>
</li>
<li>优点<ul>
<li>朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率</li>
<li>对缺失数据不太敏感，算法也较简单，常用于文本分类</li>
<li>分类准确度高，速度快</li>
</ul>
</li>
<li>缺点<ul>
<li>由于使用了样本属性独立性的假说，所以如果特征属性有关联时效果不好</li>
</ul>
</li>
</ol>
<h3 id="3-决策树"><a href="#3-决策树" class="headerlink" title="3. 决策树"></a>3. 决策树</h3><ol>
<li><p>思想与原理   </p>
<ul>
<li>if-else，特征有优先级</li>
<li>原理：信息熵，信息增益等，需要信息论知识</li>
</ul>
</li>
<li><p>信息论基础<br> (1) 信息</p>
<pre><code> - 香农：消除随即不定性的东西
 - 例子：小明：“我今年18岁了”，可以得到一条信息。小华再说：“小明明年19岁”。这句话就不是信息了，是确定的东西，信息熵为0</code></pre>
<p> (2) 信息的衡量(消除不确定性)</p>
<pre><code> - 信息量
 - 信息熵</code></pre>
<p> (3) 信息增益</p>
<pre><code> - 决策树的划分依据之一，值越大，特征优先级越高，还有其他划分依据：信息增益比，基尼系数等</code></pre>
</li>
<li><p>优点</p>
<ul>
<li>理解简单，树木可视化</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为过拟合</li>
</ul>
</li>
<li><p>改进</p>
<ul>
<li>剪枝cart算法</li>
<li>随即森林</li>
</ul>
</li>
<li><p>应用</p>
<ul>
<li>企业重要决策</li>
</ul>
</li>
</ol>
<h3 id="4-随即森林"><a href="#4-随即森林" class="headerlink" title="4. 随即森林"></a>4. 随即森林</h3><ol>
<li>集成学习方法<ul>
<li>通过建立几个模型组合的来解决单一预测问题</li>
<li>生成多个分类器/模型，各自独立的学习和做出预测。这些预测最后组合城组合预测，因此优于任何一个单分类的预测</li>
</ul>
</li>
<li>随即森林<ul>
<li>包含多个决策树的分类器，属于集成学习方法之一</li>
</ul>
</li>
<li>随即<ul>
<li>训练集随机，从原有N个样本中随即有放回的抽样N个<ul>
<li>为什么随即抽样？为了让各个树不一样</li>
<li>问什么有放回？若不放回，各个树没有交集，每棵树都是“有偏的”“片面的”，各个树训练出来的差异大</li>
</ul>
</li>
<li>特征随即，从M个特征中随即抽取m个特征，M&gt;&gt;m,降维</li>
</ul>
</li>
<li>优点<ul>
<li>在当前算法中，具有较好的准确率</li>
<li>能有效运行在大数据集上，处理具有高纬度特征的样本，而且不需要降维</li>
<li>能评估各个特征在分类问题上的重要性</li>
</ul>
</li>
</ol>
<h1 id="四、回归与聚类算法"><a href="#四、回归与聚类算法" class="headerlink" title="四、回归与聚类算法"></a>四、回归与聚类算法</h1><h3 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h3><ol>
<li>定义：目标值是连续的数据</li>
<li>通用公式：h(w)=w1x1+w2x2+…+b=Wx+b</li>
<li>线性模型<ul>
<li>线性关系：x的次数都为1</li>
<li>非线性关系：x的次数不定，如x+x^2+x^3</li>
</ul>
</li>
<li>线性回归的损失和优化原理<ul>
<li>目标：求模型参数</li>
<li>损失函数/cost/成本函数/目标函数<ul>
<li>最小二乘法</li>
</ul>
</li>
<li>优化算法<ul>
<li>正规方程：直接求W,当特征值过多过复杂时，求解速度过慢且得不到正确结果，不能解决过拟合的问题</li>
<li>梯度下降：不断试错改进，需要手动指定学习速率(超参数)</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="2-欠拟合与过拟合"><a href="#2-欠拟合与过拟合" class="headerlink" title="2. 欠拟合与过拟合"></a>2. 欠拟合与过拟合</h3><ol>
<li>过拟合<ul>
<li>训练集效果好，测试集效果不好</li>
</ul>
</li>
<li>欠拟合<ul>
<li>学习特征过少</li>
</ul>
</li>
<li>解决方案<ul>
<li>L1正则化：使一些w直接为0，删除影响，又名LASSO回归</li>
<li>L2正则化：更常用，使得其中一些w很小，接近0，削弱某个特征的影响，又名岭回归。损失函数+惩罚项</li>
</ul>
</li>
</ol>
<h3 id="3-岭回归"><a href="#3-岭回归" class="headerlink" title="3. 岭回归"></a>3. 岭回归</h3><ul>
<li>就是用了L2正则化的线性回归</li>
</ul>
<h3 id="4-分类算法–逻辑回归与二分类"><a href="#4-分类算法–逻辑回归与二分类" class="headerlink" title="4. 分类算法–逻辑回归与二分类"></a>4. 分类算法–逻辑回归与二分类</h3><ol>
<li>应用场景<ul>
<li>如是否是垃圾邮件，是否患病等</li>
</ul>
</li>
<li>原理<ul>
<li>输入：逻辑回归的输入就是线性回归的输出结果，h(w)=w1x1+w2x2+…+b=Wx+b</li>
<li>激活函数：sigmoid函数</li>
<li>分析：将回归的结果输入到sigmoid函数中，输出结果为[0,1]间的一个概率值，默认为0.5为阈值，大于这个阈值属于一个类别，小于阈值属于另一个类别</li>
</ul>
</li>
<li>损失函数<ul>
<li>其值越小越好</li>
<li>因为实际分类为1或0,最小二乘法适用于连续数据，在这里不适用，要用对数似然损失</li>
</ul>
</li>
<li>优化损失<ul>
<li>梯度下降</li>
</ul>
</li>
</ol>
<h3 id="5-分类的评估方法"><a href="#5-分类的评估方法" class="headerlink" title="5. 分类的评估方法"></a>5. 分类的评估方法</h3><table>
<thead>
<tr>
<th>真实结果\预测结果</th>
<th>正例</th>
<th>假例</th>
</tr>
</thead>
<tbody><tr>
<td>正例</td>
<td>真正例TP</td>
<td>伪反例FN</td>
</tr>
<tr>
<td>假例</td>
<td>伪正例FP</td>
<td>真反例TN</td>
</tr>
</tbody></table>
<ol>
<li>精确率<ul>
<li>预测结果为正例样本中，真正为正例的比例</li>
<li>TP/(TP+FP)</li>
</ul>
</li>
<li>召回率<ul>
<li>真实为正例的样本中预测结果为正例的比例（对正样本的区分能力，常用）</li>
<li>TP/(TP+FN)</li>
</ul>
</li>
<li>F1-score<ul>
<li>反应模型的文件稳健性</li>
</ul>
</li>
</ol>
<h3 id="6-ROC曲线与AUC指标–衡量样本不均衡下的评估"><a href="#6-ROC曲线与AUC指标–衡量样本不均衡下的评估" class="headerlink" title="6. ROC曲线与AUC指标–衡量样本不均衡下的评估"></a>6. ROC曲线与AUC指标–衡量样本不均衡下的评估</h3><ol>
<li>ROC曲线<ul>
<li>TPR为纵轴，TPR是召回率</li>
<li>FPR为横轴，TPR是在所有真实类别为假的样本中，预测为假的比例</li>
</ul>
</li>
<li>AUC指标<ul>
<li>就是ROC曲线下方的面积</li>
<li>范围[0.5,1],若是0.5,相当于瞎猜</li>
<li>AUC只能用来评价二分类</li>
<li>AUC非常适合评价样本不均衡中的分类器性能</li>
</ul>
</li>
</ol>
<h1 id="五、无监督学习–K-means算法"><a href="#五、无监督学习–K-means算法" class="headerlink" title="五、无监督学习–K-means算法"></a>五、无监督学习–K-means算法</h1><h3 id="1-什么是无监督学习"><a href="#1-什么是无监督学习" class="headerlink" title="1. 什么是无监督学习"></a>1. 什么是无监督学习</h3><ul>
<li>没有目标值</li>
</ul>
<h3 id="2-无监督学习算法"><a href="#2-无监督学习算法" class="headerlink" title="2. 无监督学习算法"></a>2. 无监督学习算法</h3><ul>
<li>聚类：K-means(K均值聚类)</li>
<li>降维：PCA</li>
</ul>
<h3 id="3-K-means原理"><a href="#3-K-means原理" class="headerlink" title="3. K-means原理"></a>3. K-means原理</h3><ol>
<li>随即设置K个特征空间内的点作为初始的聚类中心</li>
<li>对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</li>
<li>接着对着标记的聚类中心后，重新计算出每个聚类的新中心点（平均值）</li>
<li>如果计算的新中心点与原中心点一致，结束。否则重复2</li>
</ol>
<h3 id="4-优点"><a href="#4-优点" class="headerlink" title="4. 优点"></a>4. 优点</h3><ul>
<li>采用迭代式算法，直观易懂且实用</li>
</ul>
<h3 id="5-缺点"><a href="#5-缺点" class="headerlink" title="5. 缺点"></a>5. 缺点</h3><ul>
<li>容易收敛到局部最优解。解决办法：多次聚类</li>
</ul>
<h3 id="6-应用场景"><a href="#6-应用场景" class="headerlink" title="6. 应用场景"></a>6. 应用场景</h3><ul>
<li>一开始没有目标值，可以先聚类获得目标值，再分类</li>
</ul>
<hr>
<h1 id="六、深度学习介绍"><a href="#六、深度学习介绍" class="headerlink" title="六、深度学习介绍"></a>六、深度学习介绍</h1><h3 id="1-深度学习与机器学习的区别"><a href="#1-深度学习与机器学习的区别" class="headerlink" title="1. 深度学习与机器学习的区别"></a>1. 深度学习与机器学习的区别</h3><ol>
<li>特征提取方面<ul>
<li>机器学习：特征工程步骤手动完成，需要大量领域专业知识</li>
<li>深度学习：通常由多个层组成。它们通常将更简单的模型组合在一起，将数据从一层传递到另一层来构建更复杂的模型。通过训练大量数据自动得出模型，不需要人工特征提取环节。<ul>
<li>深度学习适合用在难提取特征的图像，语音，自然语言处理领域</li>
</ul>
</li>
</ul>
</li>
<li>数据量和计算性能要求<ul>
<li>深度学习需要大量的训练数据</li>
<li>训练深度神经网络需要大量的算力<ul>
<li>所有深度学习通常需要强大的GPU服务器来计算</li>
<li>需要全面管理的分布式训练与预测服务——如谷歌Tensorflow云机器学习平台</li>
</ul>
</li>
</ul>
</li>
<li>算法代表<ul>
<li>机器学习：朴素贝叶斯，决策树等</li>
<li>深度学习：神经网络</li>
</ul>
</li>
</ol>
<h3 id="2-深度学习应用场景"><a href="#2-深度学习应用场景" class="headerlink" title="2. 深度学习应用场景"></a>2. 深度学习应用场景</h3><ol>
<li>图像识别<ul>
<li>物体识别，场景识别，车型识别，人脸检测跟踪，人脸关键点定位，人脸身份认证</li>
</ul>
</li>
<li>自然语言处理<ul>
<li>机器翻译，文本识别，聊天对话</li>
</ul>
</li>
<li>语言技术<ul>
<li>语音识别</li>
</ul>
</li>
</ol>
<h3 id="GPU-CPU"><a href="#GPU-CPU" class="headerlink" title="GPU/CPU"></a>GPU/CPU</h3><ol>
<li>CPU<ul>
<li>核心数量少，但每一个核心的速度更快，性能更强</li>
<li>更适用于处理连续性任务(sequential)</li>
</ul>
</li>
<li>GPU<ul>
<li>核心数量多,但每一个核心的速度慢</li>
<li>更适用于处理并行任务(parallel)</li>
</ul>
</li>
</ol>
<h1 id="七、Tensorflow框架"><a href="#七、Tensorflow框架" class="headerlink" title="七、Tensorflow框架"></a>七、Tensorflow框架</h1><h3 id="1-Tensorflow结构"><a href="#1-Tensorflow结构" class="headerlink" title="1. Tensorflow结构"></a>1. Tensorflow结构</h3><ol>
<li>构建阶段：数据（张量）与操作（节点）的执行步骤描述成一个图</li>
<li>执行阶段：使用会话执行构建好的图中的操作。session</li>
<li>数据流图<ul>
<li>Tensor——数据——张量</li>
<li>Flow——操作——节点</li>
</ul>
</li>
</ol>
<h3 id="2-会话"><a href="#2-会话" class="headerlink" title="2. 会话"></a>2. 会话</h3><ol>
<li>创建会话，常用tf.session</li>
<li>会话初始化参数</li>
<li>会话的执行——run(op,feed_dict=None)<ul>
<li>op：单一的operation操作</li>
<li>feed_dict：传入的数据，与tf.placeholder()搭配</li>
</ul>
</li>
</ol>
<h3 id="3-张量"><a href="#3-张量" class="headerlink" title="3. 张量"></a>3. 张量</h3><ol>
<li>定义<ul>
<li>就是数据的定义</li>
<li>n维数组</li>
</ul>
</li>
<li>创建张量<ul>
<li>tf.zeros(),全0</li>
<li>tf.ones()，全1</li>
<li>tf.constrant(),常量</li>
<li>tf.random_normal()，随即</li>
<li>tf.Varinable()，变量</li>
<li>tf.placeholder(),占位符</li>
</ul>
</li>
<li>张量变化<ul>
<li>类型改变</li>
<li>形状改变</li>
<li>张量的数学运算</li>
</ul>
</li>
</ol>
<h3 id="4-变量op-操作"><a href="#4-变量op-操作" class="headerlink" title="4. 变量op(操作)"></a>4. 变量op(操作)</h3><ol>
<li>创建变量<ul>
<li>tf.Variable()</li>
<li>变量需要显式初始化，才能运行值,如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init&#x3D;tf.global_variables_initializer()</span><br><span class="line">with ... as sess:</span><br><span class="line">    sess.run(init)</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ol>
<h3 id="5-模型保存与加载"><a href="#5-模型保存与加载" class="headerlink" title="5. 模型保存与加载"></a>5. 模型保存与加载</h3><ul>
<li>用时再搜</li>
</ul>
<h3 id="6-数据IO"><a href="#6-数据IO" class="headerlink" title="6. 数据IO"></a>6. 数据IO</h3><h5 id="1-三种获取数据到Tensorflow程序的方式"><a href="#1-三种获取数据到Tensorflow程序的方式" class="headerlink" title="1. 三种获取数据到Tensorflow程序的方式"></a>1. 三种获取数据到Tensorflow程序的方式</h5><pre><code>- QueueRunner:基于队列的输入管道从Tensorflow图形开头的文件中读取数据（最高效）
- Feeding：运行每一步时，Python代码提供数据，和placeholder()结合使用
- 预加载数据：Tensorflow图中的张量包含所有数据(对于小数据集)</code></pre>
<h5 id="2-文件读取流程"><a href="#2-文件读取流程" class="headerlink" title="2. 文件读取流程"></a>2. 文件读取流程</h5><ol>
<li><p>构建文件名队列</p>
<ul>
<li>tf.train.string_input_producer(string_tensor,shuffle=True)</li>
<li>string_tensor为列表</li>
</ul>
</li>
<li><p>读取</p>
<ul>
<li>tf.TextLineReader,读csv格式文件</li>
<li>tf.WholeFileReader,读取图片文件</li>
<li>tf.TFRecordReader,读取TFRecords文件</li>
<li>用时再查</li>
</ul>
</li>
<li><p>解码</p>
<ul>
<li>读取不同类型的文件，要进行相应的解码操作，解码成统一的Tensor格式</li>
<li>tf.decode_csv,解码csv文本文件</li>
<li>tf.image.decode_jpeg(contents),将JPEG编码的图像解码为uint8张量</li>
<li>tf.image.decode_png(contents),将PNG编码的图像解码为uint8张量</li>
<li>tf.decode_raw:解码二进制文件内容</li>
</ul>
</li>
<li><p>批处理</p>
<ul>
<li>解码后只获得默认的一个样本内容，若想获取多个样本，需要加入到新的队列进行批处理</li>
<li>tf.train.batch</li>
<li>tf.train.shuffle_batch</li>
</ul>
</li>
</ol>
<h3 id="7-TensorBoard可视化"><a href="#7-TensorBoard可视化" class="headerlink" title="7. TensorBoard可视化"></a>7. TensorBoard可视化</h3><ol>
<li><p>图结构</p>
<ul>
<li>数据+操作</li>
</ul>
</li>
<li><p>图相关操作</p>
<ul>
<li>默认图<ul>
<li>查看默认图：tf.get_default_graph()</li>
<li>查看属性：sess.graph</li>
</ul>
</li>
<li>创建图  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_g&#x3D;tf.Graph()</span><br><span class="line">with new_g.as_default():</span><br><span class="line">    定义数据和操作</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>可视化</p>
<ul>
<li>数据序列化为events文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.FileWriter(path&#x3D;&quot;写入路径&quot;,graph&#x3D;sess.graph) # 第二个为图的名字</span><br></pre></td></tr></table></figure></li>
<li>查看图<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;在命令行输入</span><br><span class="line">tensorboard --logdir&#x3D;&quot;图的目录&quot;</span><br><span class="line">&#x2F;&#x2F;在浏览器输入</span><br><span class="line">127.0.0.1:6006</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ol>
<h5 id="3-图片数据"><a href="#3-图片数据" class="headerlink" title="3. 图片数据"></a>3. 图片数据</h5><ol>
<li>用张量表示一张图片<ul>
<li>张量形状：Tensor(指令名称，shape,dtype)</li>
<li>对于shape<ul>
<li>一张图片：shape=(height,width,channels通道数)</li>
<li>多张图片：shape=(batch,…如上)，batch表示一个批次的张量数量</li>
</ul>
</li>
</ul>
</li>
<li>图片特征值处理<ul>
<li>若图片大小不一，要进行统一的缩放处理，保持特征数量相同</li>
<li>方法：tf.image.resize_images(images,size)</li>
<li>数据格式：<ul>
<li>存储：uint8</li>
<li>矩阵计算：float32</li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="八、神经网络"><a href="#八、神经网络" class="headerlink" title="八、神经网络"></a>八、神经网络</h1><h3 id="1-人工神经网络-Artificial-Neural-Network-ANN"><a href="#1-人工神经网络-Artificial-Neural-Network-ANN" class="headerlink" title="1. 人工神经网络(Artificial Neural Network,ANN)"></a>1. 人工神经网络(Artificial Neural Network,ANN)</h3><ol>
<li>简称神经网络(NN)，经典NN结构包含三个层次的神经网络，分别为输入层，隐藏层，输出层。</li>
<li>感知机<ul>
<li>一种最基础的分类模型，类似于逻辑回归</li>
<li>和逻辑回归激活函数不同，逻辑回归是sigmoid，感知机是sign</li>
<li>感知机只能解决简单的与或问题</li>
</ul>
</li>
</ol>
<h3 id="2-神经网络原理"><a href="#2-神经网络原理" class="headerlink" title="2. 神经网络原理"></a>2. 神经网络原理</h3><ol>
<li>神经网络的主要用途在于分类，分类的原理要围绕损失，优化两块</li>
<li>softmax回归<ul>
<li>将神经网络输出转换成概率结果。类似与逻辑回归中的sigmoid函数，sigmoid输出的是某个类别的概率</li>
<li>logits+softmax能解决多分类问题</li>
</ul>
</li>
<li>交叉熵损失(损失函数)<ul>
<li>真实值要用one-hot形式[0,0,0,0,1,0,0,0],这样才能算离真实值的距离</li>
<li>损失大小<ul>
<li>总损失——求平均</li>
<li>最小二乘法——衡量线性回归的损失——均方误差</li>
</ul>
</li>
<li>优化损失函数</li>
</ul>
</li>
</ol>
<h3 id="3-手写数字识别例子"><a href="#3-手写数字识别例子" class="headerlink" title="3. 手写数字识别例子"></a>3. 手写数字识别例子</h3><ol>
<li>准备数据</li>
<li>构建模型</li>
<li>构造损失函数</li>
<li>优化损失</li>
</ol>
<h3 id="4-卷积神经网络-CNN"><a href="#4-卷积神经网络-CNN" class="headerlink" title="4. 卷积神经网络(CNN)"></a>4. 卷积神经网络(CNN)</h3><h5 id="1-CNN和NN的区别"><a href="#1-CNN和NN的区别" class="headerlink" title="1. CNN和NN的区别"></a>1. CNN和NN的区别</h5><ol>
<li>NN<ul>
<li>只有输入层，隐藏层，输出层。</li>
<li>隐藏层的层数根据需要而定，没有明确说多少层合适</li>
</ul>
</li>
<li>CNN<ul>
<li>在原多层网络的基础上，加入了更有效的特征学习部分，具体操作如下<ul>
<li>在原来的全连接层前加入了卷积层和池化层</li>
</ul>
</li>
<li>卷积神经网络出现，使得神经网络层数得以加深，“深度”学习由此而来</li>
<li>2012年被重视</li>
</ul>
</li>
<li>深度学习<ul>
<li>一般指的是这些CNN等新的结构以及一些新的方法(比如一些新的激活函数Relu等)，解决了传统多层神经网络的一些难以解决的问题。</li>
</ul>
</li>
</ol>
<h5 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2. 卷积层"></a>2. 卷积层</h5><ol>
<li>通过在原始图像上平移来提取特征</li>
<li>卷积核（Filter过滤器）四大要素<ul>
<li>卷积核个数</li>
<li>大小，3*3,5*5</li>
<li>步长</li>
<li>零填充大小</li>
</ul>
</li>
<li>卷积运算的目的是提取特征</li>
<li>每层卷积层由若干卷积单元(卷积核)组成</li>
<li>每个卷积单元的参数是通过反向传播算法以最佳化得到的</li>
</ol>
<h5 id="3-激活层"><a href="#3-激活层" class="headerlink" title="3. 激活层"></a>3. 激活层</h5><ol>
<li>使用激活函数，增加非线性分割能力</li>
<li>sigmoid不能满足，效果不好，计算量大，且反向传播时，容易出现梯度消失</li>
<li>Relu<ul>
<li>解决梯度消失问题</li>
<li>计算速度快</li>
<li>对图像没有负的像素值</li>
</ul>
</li>
</ol>
<h5 id="4-池化层pooling"><a href="#4-池化层pooling" class="headerlink" title="4. 池化层pooling"></a>4. 池化层pooling</h5><ol>
<li>主要作用是特征提取。通过去掉Feature Map中不重要的样本，进一步减少参数数量，降低网络的复杂度，防止过拟合。</li>
</ol>
<h5 id="5-全连接层"><a href="#5-全连接层" class="headerlink" title="5. 全连接层"></a>5. 全连接层</h5><ol>
<li>前面的卷积和池化相当于做特征工程</li>
<li>最后的全连接层在整个CNN中起到“分类器”的作用，最后的输出层，进行损失计算</li>
</ol>
<h3 id="5-网络设计"><a href="#5-网络设计" class="headerlink" title="5. 网络设计"></a>5. 网络设计</h3><h5 id="1-一个简单的网络结构"><a href="#1-一个简单的网络结构" class="headerlink" title="1. 一个简单的网络结构"></a>1. 一个简单的网络结构</h5><ul>
<li>卷积|激活|池化—&gt;卷积|激活|池化—&gt;全连接</li>
</ul>
<h5 id="2-具体参数"><a href="#2-具体参数" class="headerlink" title="2. 具体参数"></a>2. 具体参数</h5><ol>
<li>第一层：<ul>
<li>卷积：<ul>
<li>输入图像(None,28,28,1)</li>
<li>32个filter,5*5,strides=1(步长)，padding=SAME</li>
<li>输出形状(None,28,28,32)</li>
</ul>
</li>
<li>激活：Relu</li>
<li>池化：大小2*2，步长2<ul>
<li>输入图像(None,28,28,32)</li>
<li>2*2,strides=2(步长)</li>
<li>输出形状(None,14,14,32)</li>
</ul>
</li>
</ul>
</li>
<li>第二层：<ul>
<li>卷积：<ul>
<li>输入图像(None,14,14，32)</li>
<li>64个filter,5*5,strides=1(步长)，padding=SAME</li>
<li>输出形状(None,14,14,64)</li>
</ul>
</li>
<li>激活：Relu</li>
<li>池化：大小2*2，步长2<ul>
<li>输入图像(None,14,14,64)</li>
<li>2*2,strides=2(步长)</li>
<li>输出形状(None,7,7,64)</li>
</ul>
</li>
</ul>
</li>
<li>全连接层<ul>
<li>[None,7,7,64]–&gt;[None,7*7*64]</li>
<li>[None,7*7*64]*[7*7*64,10]=[None,10]</li>
<li>[None,1024]*[1024,10]=[None,10]</li>
<li>y_predict=tf.matmul(pool2,weights)+bias</li>
</ul>
</li>
</ol>
<h5 id="3-调参提高准确率"><a href="#3-调参提高准确率" class="headerlink" title="3. 调参提高准确率"></a>3. 调参提高准确率</h5><ol>
<li>改变学习率，梯度下降的学习率</li>
<li>随即初始化的权重，偏置的值</li>
<li>选择好的优化器，梯度下降用的</li>
</ol>
<h5 id="4-CNN几种经典模型"><a href="#4-CNN几种经典模型" class="headerlink" title="4. CNN几种经典模型"></a>4. CNN几种经典模型</h5><ol>
<li>AlexNet</li>
<li>ResNet</li>
<li>GoogleNet</li>
</ol>
<h3 id="6-MNIST例子"><a href="#6-MNIST例子" class="headerlink" title="6. MNIST例子"></a>6. MNIST例子</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">from tensorflow.python.framework import graph_util</span><br><span class="line">from tensorflow.python.saved_model import builder as saved_model_builder</span><br><span class="line">from tensorflow.python.saved_model import (signature_constants, signature_def_utils, tag_constants, utils)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot; </span><br><span class="line">0 表示训练并存成ckpt和pb</span><br><span class="line">1 表示读取ckpt测试</span><br><span class="line">2 表示存成pb文件</span><br><span class="line">3 表示读取pb测试</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">choise&#x3D;0</span><br><span class="line"></span><br><span class="line"># 加载数据，没有的话会自动下载</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist&#x3D;input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line">##################### build net model ##########################</span><br><span class="line"></span><br><span class="line">########## define conv process ##########</span><br><span class="line"># 2维卷积</span><br><span class="line">def conv2d(name,x,W,b,strides&#x3D;1, padding&#x3D;&#39;SAME&#39;):</span><br><span class="line">	x&#x3D;tf.nn.conv2d(x,W,strides&#x3D;[1,strides,strides,1],padding&#x3D;padding)</span><br><span class="line">	x&#x3D;tf.nn.bias_add(x,b)</span><br><span class="line">	return tf.nn.relu(x,name&#x3D;name)</span><br><span class="line"></span><br><span class="line">########## define pool process ##########</span><br><span class="line"># 池化</span><br><span class="line">def maxpool2d(name, x, k&#x3D;3, s&#x3D;2, padding&#x3D;&#39;SAME&#39;):</span><br><span class="line">	return tf.nn.max_pool(x,ksize&#x3D;[1,k,k,1],strides&#x3D;[1,s,s,1],padding&#x3D;padding,name&#x3D;name)</span><br><span class="line"></span><br><span class="line">########## set net parameters ##########</span><br><span class="line">def weight_var2(shape):</span><br><span class="line">	return tf.Variable(initial_value&#x3D;tf.random_normal(shape&#x3D;shape))	# 正态分布随机数初始化</span><br><span class="line">	</span><br><span class="line">def weight_var(shape):</span><br><span class="line">	initial &#x3D; tf.truncated_normal(shape, stddev&#x3D;0.1)</span><br><span class="line">	return tf.Variable(initial)</span><br><span class="line">def bias_var(shape):</span><br><span class="line">	initial &#x3D; tf.constant(0.1, shape&#x3D;shape)</span><br><span class="line">	return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot; </span><br><span class="line">1. 第一层：</span><br><span class="line">    - 卷积：</span><br><span class="line">        - 输入图像(None,28,28,1)</span><br><span class="line">        - 32个filter,5*5,strides&#x3D;1(步长)，padding&#x3D;SAME</span><br><span class="line">        - 输出形状(None,28,28,32)</span><br><span class="line">    - 激活：Relu</span><br><span class="line">    - 池化：大小2*2，步长2</span><br><span class="line">        - 输入图像(None,28,28,32)</span><br><span class="line">        - 2*2,strides&#x3D;2(步长)</span><br><span class="line">        - 输出形状(None,14,14,32)</span><br><span class="line">2. 第二层：</span><br><span class="line">    - 卷积：</span><br><span class="line">        - 输入图像(None,14,14，32)</span><br><span class="line">        - 64个filter,5*5,strides&#x3D;1(步长)，padding&#x3D;SAME</span><br><span class="line">        - 输出形状(None,14,14,64)</span><br><span class="line">    - 激活：Relu</span><br><span class="line">    - 池化：大小2*2，步长2</span><br><span class="line">        - 输入图像(None,14,14,64)</span><br><span class="line">        - 2\*2,strides&#x3D;2(步长)</span><br><span class="line">        - 输出形状(None,7,7,64)</span><br><span class="line">3. 全连接层</span><br><span class="line">    - [None,7,7,64]--&gt;[None,7*7*64]</span><br><span class="line">    - [None,7*7*64]*[7*7*64,1024]&#x3D;[None,1024]</span><br><span class="line">	- [None,1024]*[1024,10]&#x3D;[None,10]</span><br><span class="line">    - y_predict&#x3D;tf.matmul(pool2,weights)+bias</span><br><span class="line"></span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line">def my_net(x):</span><br><span class="line">	&quot;&quot;&quot; </span><br><span class="line">	卷积神经网络</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	y_pred&#x3D;0</span><br><span class="line">	# 第一层</span><br><span class="line">	# 卷积层</span><br><span class="line">	# 将x[None,784]形状修改</span><br><span class="line">	# shape第一个是载入样本数量，不知道一次性载入多少，所以用-1,1为通道数</span><br><span class="line">	input_x &#x3D; tf.reshape(x,shape&#x3D;[-1,28,28,1])</span><br><span class="line">	# 定义filter和偏置</span><br><span class="line">	conv1_weights &#x3D; weight_var2(shape&#x3D;[5,5,1,32])	# 5*5的filter,通道数为1,32个filter</span><br><span class="line">	conv1_bias &#x3D; weight_var2(shape&#x3D;[32])</span><br><span class="line">	conv1_x &#x3D; tf.nn.conv2d(input&#x3D;input_x,filter&#x3D;conv1_weights,strides&#x3D;[1,1,1,1],padding&#x3D;&quot;SAME&quot;)+conv1_bias	#步长为[batch, height, width, channels]这样的shape，一般为[1,s,s,1],s表示步长</span><br><span class="line">																											#padding为0填充，SAME表示输入图片大小和输出图片大小是一致的，如果是VALID则图片经过滤波器后可能会变小。</span><br><span class="line">	# 激活层</span><br><span class="line">	relu1_x&#x3D;tf.nn.relu(conv1_x)</span><br><span class="line">	# 池化层</span><br><span class="line">	pool1_x &#x3D; tf.nn.max_pool(value&#x3D;relu1_x,ksize&#x3D;[1,2,2,1],strides&#x3D;[1,2,2,1],padding&#x3D;&quot;SAME&quot;)	# ksize为池化窗口的大小，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1</span><br><span class="line">																								</span><br><span class="line">	# 第二层</span><br><span class="line">	# 定义filter和偏置</span><br><span class="line">	conv2_weights &#x3D; weight_var2(shape&#x3D;[5,5,32,64])	# 5*5的filter,通道数为1,32个filter</span><br><span class="line">	conv2_bias &#x3D; weight_var2(shape&#x3D;[64])</span><br><span class="line">	conv2_x &#x3D; tf.nn.conv2d(input&#x3D;pool1_x,filter&#x3D;conv2_weights,strides&#x3D;[1,1,1,1],padding&#x3D;&quot;SAME&quot;)+conv2_bias																											</span><br><span class="line">	# 激活层</span><br><span class="line">	relu2_x&#x3D;tf.nn.relu(conv2_x)</span><br><span class="line">	# 池化层</span><br><span class="line">	pool2_x &#x3D; tf.nn.max_pool(value&#x3D;relu2_x,ksize&#x3D;[1,2,2,1],strides&#x3D;[1,2,2,1],padding&#x3D;&quot;SAME&quot;)</span><br><span class="line"></span><br><span class="line">	# 全连接层</span><br><span class="line">	&quot;&quot;&quot; </span><br><span class="line">		[None,7,7,64]--&gt;[None,7*7*64]</span><br><span class="line">    	[None,7*7*64]*[7*7*64,10]&#x3D;[None,10]</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	x_fc&#x3D;tf.reshape(pool2_x,shape&#x3D;[-1,7*7*64])</span><br><span class="line">	weights_fc&#x3D;weight_var(shape&#x3D;[7*7*64,10])</span><br><span class="line">	bias_fc&#x3D;weight_var(shape&#x3D;[10])</span><br><span class="line">	y_pred&#x3D;tf.matmul(x_fc,weights_fc)+bias_fc</span><br><span class="line">	# y_pred2&#x3D;tf.nn.softmax(tf.matmul(x_fc, weights_fc) + bias_fc,name&#x3D;&quot;final_result&quot;)</span><br><span class="line">	return y_pred</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">		</span><br><span class="line">	########## placeholder ##########</span><br><span class="line">	x&#x3D;tf.placeholder(&quot;float&quot;,[None,784],name&#x3D;&quot;x&quot;)</span><br><span class="line">	y_true&#x3D;tf.placeholder(&quot;float&quot;,[None,10],name&#x3D;&quot;y_true&quot;)</span><br><span class="line"></span><br><span class="line">	W &#x3D; tf.Variable(tf.zeros([784,10]))</span><br><span class="line">	b &#x3D; tf.Variable(tf.zeros([10]))</span><br><span class="line"></span><br><span class="line">	#### model ####	</span><br><span class="line">	y_pred &#x3D; my_net(x)</span><br><span class="line"></span><br><span class="line">	# softmax 回归及交叉熵损失计算</span><br><span class="line">	# labels 真实值</span><br><span class="line">	# logits 预测值</span><br><span class="line">	loss&#x3D;tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels&#x3D;y_true,logits&#x3D;y_pred))</span><br><span class="line">	loss2&#x3D;tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels&#x3D;y_true,logits&#x3D;y_pred,name&#x3D;&#39;final_result&#39;))</span><br><span class="line">	</span><br><span class="line">	# tf.add_to_collection(&#39;output&#39;, y_conv2)</span><br><span class="line">	#梯度下降损失优化</span><br><span class="line">	train_step &#x3D; tf.train.GradientDescentOptimizer(0.001).minimize(loss)</span><br><span class="line">	# train_step &#x3D; tf.train.AdamOptimizer(0.001).minimize(cross_entropy)	# 这个比梯度下降损失优化差一点</span><br><span class="line">	correct_prediction &#x3D; tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))</span><br><span class="line">	accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</span><br><span class="line">	########## initialize variables ##########</span><br><span class="line">	init&#x3D;tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">	saver&#x3D;tf.train.Saver(max_to_keep&#x3D;1)</span><br><span class="line">	</span><br><span class="line">	with tf.Session() as sess:</span><br><span class="line">		sess.run(init)</span><br><span class="line">		if(choise&#x3D;&#x3D;0):	# 训练阶段</span><br><span class="line">			max_acc&#x3D;0</span><br><span class="line">			for i in range(1000):</span><br><span class="line">				mnist_x,mnist_y &#x3D; mnist.train.next_batch(50)</span><br><span class="line">				sess.run(train_step,feed_dict&#x3D;&#123;x:mnist_x,y_true:mnist_y&#125;)</span><br><span class="line">				val_loss,val_acc&#x3D;sess.run([loss,accuracy], feed_dict&#x3D;&#123;x: mnist_x, y_true: mnist_y&#125;)</span><br><span class="line">				# val_loss,val_acc&#x3D;sess.run([loss,accuracy], feed_dict&#x3D;&#123;x: mnist.test.images, y_true: mnist.test.labels&#125;)</span><br><span class="line">				if val_acc&gt;max_acc:</span><br><span class="line">					max_acc&#x3D;val_acc</span><br><span class="line">					saver.save(sess,&#39;ckpt&#x2F;mnist.ckpt&#39;,global_step&#x3D;i+1)	#global_step是一个计数器，可以用于保存多个中间结果</span><br><span class="line">				if(i%50&#x3D;&#x3D;0):</span><br><span class="line">					print(&quot;准确率为：&#123;0&#125;, 损失为：&#123;1&#125;&quot;.format(val_acc,val_loss))</span><br><span class="line">			# # 保存图</span><br><span class="line">			# # tensorboard --logdir&#x3D;&quot;path&quot; 再通过浏览器查看图</span><br><span class="line">			# tf.summary.FileWriter(&quot;.&#x2F;graph&#x2F;one&quot;,graph&#x3D;sess.graph)</span><br><span class="line">			# # 保存为pb文件</span><br><span class="line">			# #将当前图设置为默认图</span><br><span class="line">			# graph_def &#x3D; tf.get_default_graph().as_graph_def() </span><br><span class="line">			# #将上面的变量转化成常量，保存模型为pb模型时需要,注意这里的final_result和前面的y_con2是同名，只有这样才会保存它，否则会报错，</span><br><span class="line">			# # 如果需要保存其他tensor只需要让tensor的名字和这里保持一直即可</span><br><span class="line">			# output_graph_def &#x3D; tf.graph_util.convert_variables_to_constants(sess,  </span><br><span class="line">			# 				graph_def, [&#39;final_result&#39;])  </span><br><span class="line">			# #保存前面训练后的模型为pb文件</span><br><span class="line">			# with tf.gfile.GFile(&quot;.&#x2F;pb_model&#x2F;three&#x2F;model.pb&quot;, &#39;wb&#39;) as f:  </span><br><span class="line">			# 		f.write(output_graph_def.SerializeToString())</span><br><span class="line"></span><br><span class="line">		elif(choise&#x3D;&#x3D;1):	# ckpt测试阶段</span><br><span class="line">			model_file&#x3D;tf.train.latest_checkpoint(&#39;ckpt&#x2F;&#39;)</span><br><span class="line">			saver.restore(sess,model_file)</span><br><span class="line">			val_loss,val_acc&#x3D;sess.run([loss,accuracy], feed_dict&#x3D;&#123;x: mnist.test.images, y_true: mnist.test.labels&#125;)</span><br><span class="line">			print(&#39;准确率为: &#123;0&#125;, 损失为： &#123;1&#125;&#39;.format(val_acc,val_loss))</span><br><span class="line">		elif(choise&#x3D;&#x3D;2):	# 训练保存为pb</span><br><span class="line">			model_file&#x3D;tf.train.latest_checkpoint(&#39;ckpt&#x2F;&#39;)</span><br><span class="line">			saver.restore(sess,model_file)</span><br><span class="line">			constant_graph &#x3D; graph_util.convert_variables_to_constants(sess, sess.graph_def, [&#39;final_result&#39;])			</span><br><span class="line">			with tf.gfile.FastGFile(&quot;.&#x2F;pb_model&#x2F;two&#x2F;mymnist_model.pb&quot;, mode&#x3D;&#39;wb&#39;) as f:</span><br><span class="line">				f.write(constant_graph.SerializeToString())</span><br><span class="line">		elif(choise&#x3D;&#x3D;3):	# pb测试</span><br><span class="line">			# load_pb()</span><br><span class="line">			pass</span><br><span class="line">		else:</span><br><span class="line">			print(&quot;choise出错&quot;)</span><br><span class="line">			</span><br></pre></td></tr></table></figure>
<ul>
<li>为了保存为pb文件在安卓上运行而找的代码<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"># 保存模型需要的库</span><br><span class="line">from tensorflow.python.framework.graph_util import convert_variables_to_constants </span><br><span class="line">from tensorflow.python.framework import graph_util </span><br><span class="line"># 导入其他库</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np </span><br><span class="line"></span><br><span class="line">#权重</span><br><span class="line">def weight_variable(shape):</span><br><span class="line">  initial &#x3D; tf.truncated_normal(shape, stddev&#x3D;0.1)</span><br><span class="line">  return tf.Variable(initial)</span><br><span class="line">#偏差</span><br><span class="line">def bias_variable(shape):</span><br><span class="line">  initial &#x3D; tf.constant(0.1, shape&#x3D;shape)</span><br><span class="line">  return tf.Variable(initial)</span><br><span class="line">#卷积</span><br><span class="line">def conv2d(x, W):</span><br><span class="line">  return tf.nn.conv2d(x, W, strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">#最大池化</span><br><span class="line">def max_pool_2x2(x):</span><br><span class="line">  return tf.nn.max_pool(x, ksize&#x3D;[1, 2, 2, 1],</span><br><span class="line">                        strides&#x3D;[1, 2, 2, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line"></span><br><span class="line">#获取MINIST数据</span><br><span class="line">mnist&#x3D;input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"># 创建会话 </span><br><span class="line">sess &#x3D; tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x &#x3D; tf.placeholder(&quot;float&quot;, shape&#x3D;[None, 784], name&#x3D;&quot;x&quot;)</span><br><span class="line">y_true &#x3D; tf.placeholder(&quot;float&quot;,shape&#x3D;[None, 10],  name&#x3D;&quot;y_true&quot;)</span><br><span class="line">W &#x3D; tf.Variable(tf.zeros([784,10]),name&#x3D;&#39;w&#39;)</span><br><span class="line">b &#x3D; tf.Variable(tf.zeros([10]),&#39;b&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 第一层，卷积激活池化</span><br><span class="line">W_conv1 &#x3D; weight_variable([5, 5, 1, 32])</span><br><span class="line">b_conv1 &#x3D; bias_variable([32])</span><br><span class="line">x_image &#x3D; tf.reshape(x, [-1,28,28,1])</span><br><span class="line">h_conv1 &#x3D; tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 &#x3D; max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line"># 第二层，卷积激活池化</span><br><span class="line">W_conv2 &#x3D; weight_variable([5, 5, 32, 64])</span><br><span class="line">b_conv2 &#x3D; bias_variable([64])</span><br><span class="line">h_conv2 &#x3D; tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 &#x3D; max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line"># 全连接层</span><br><span class="line">W_fc1 &#x3D; weight_variable([7 * 7 * 64, 1024])</span><br><span class="line">b_fc1 &#x3D; bias_variable([1024])</span><br><span class="line">h_pool2_flat &#x3D; tf.reshape(h_pool2, [-1, 7*7*64])</span><br><span class="line">h_fc1 &#x3D; tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"># dropout防止过拟合</span><br><span class="line"># keep_prob &#x3D; tf.placeholder(&quot;float&quot;,name&#x3D;&#39;rob&#39;)</span><br><span class="line"># h_fc1_drop &#x3D; tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line">#用于训练用的softmax函数</span><br><span class="line">W_fc2 &#x3D; weight_variable([1024, 10])</span><br><span class="line">b_fc2 &#x3D; bias_variable([10])</span><br><span class="line">y_conv&#x3D;tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2,name&#x3D;&#39;res&#39;)</span><br><span class="line">#用于训练作完后，作测试用的softmax函数</span><br><span class="line">y_conv2&#x3D;tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2,name&#x3D;&quot;final_result&quot;)</span><br><span class="line"></span><br><span class="line">#交叉熵的计算，返回包含了损失值的Tensor。</span><br><span class="line">cross_entropy &#x3D; -tf.reduce_sum(y_true*tf.log(y_conv))</span><br><span class="line"></span><br><span class="line">#优化器，负责最小化交叉熵</span><br><span class="line">train_step &#x3D; tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span><br><span class="line">#梯度下降损失优化</span><br><span class="line"># train_step &#x3D; tf.train.GradientDescentOptimizer(0.001).minimize(loss)</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y_conv,1), tf.argmax(y_true,1))</span><br><span class="line">#计算准确率</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</span><br><span class="line">#初始化所以变量</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"> # 保存输入输出，可以为之后用</span><br><span class="line"># tf.add_to_collection(&#39;res&#39;, y_conv)</span><br><span class="line"># tf.add_to_collection(&#39;output&#39;, y_conv2)</span><br><span class="line"># tf.add_to_collection(&#39;result&#39;, res)</span><br><span class="line"># tf.add_to_collection(&#39;x&#39;, x)</span><br><span class="line"></span><br><span class="line">#训练开始</span><br><span class="line">for i in range(200):</span><br><span class="line">  batch &#x3D; mnist.train.next_batch(50)</span><br><span class="line">  if i%100 &#x3D;&#x3D; 0:</span><br><span class="line">    train_accuracy &#x3D; accuracy.eval(feed_dict&#x3D;&#123;</span><br><span class="line">        x:batch[0], y_true: batch[1]&#125;)</span><br><span class="line">    print(&quot;step &#123;0&#125;, training accuracy &#123;1&#125;&quot;.format(i, train_accuracy))</span><br><span class="line">#run()可以看做输入相关值给到函数中的占位符，然后计算的出结果，这里将batch[0]，给xbatch[1]给y_</span><br><span class="line">  train_step.run(feed_dict&#x3D;&#123;x: batch[0], y_true: batch[1]&#125;)</span><br><span class="line"></span><br><span class="line">#将当前图设置为默认图</span><br><span class="line">graph_def &#x3D; tf.get_default_graph().as_graph_def() </span><br><span class="line">#将上面的变量转化成常量，保存模型为pb模型时需要,注意这里的final_result和前面的y_con2是同名，只有这样才会保存它，否则会报错，</span><br><span class="line"># 如果需要保存其他tensor只需要让tensor的名字和这里保持一直即可</span><br><span class="line">output_graph_def &#x3D; tf.graph_util.convert_variables_to_constants(sess,  </span><br><span class="line">                graph_def, [&#39;final_result&#39;])  </span><br><span class="line">#保存前面训练后的模型为pb文件</span><br><span class="line">with tf.gfile.GFile(&quot;.&#x2F;model&#x2F;pb&#x2F;mnist.pb&quot;, &#39;wb&#39;) as f:  </span><br><span class="line">        f.write(output_graph_def.SerializeToString())</span><br><span class="line">print(&quot;保存pb成功&quot;)</span><br><span class="line"></span><br><span class="line">#保存模型</span><br><span class="line">saver &#x3D; tf.train.Saver()   </span><br><span class="line">saver.save(sess, &quot;.&#x2F;model&#x2F;ckpt&#x2F;mnist.ckpt&quot;)  </span><br><span class="line">print(&quot;保存ckpt成功&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag">python</i></a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag">机器学习</i></a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/%E3%80%8A%E7%B2%BE%E9%80%9ADjango%E3%80%8B/" rel="prev" title="《精通Django》">
      <i class="fa fa-chevron-left"></i> 《精通Django》
    </a></div>
      <div class="post-nav-item">
    <a href="/%E3%80%8AC-Primer-Plus%E3%80%8B/" rel="next" title="《C Primer Plus》">
      《C Primer Plus》 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BD%AE%E9%A1%B6"><span class="nav-text">置顶</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="nav-text">一、机器学习概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB"><span class="nav-text">1. 机器学习算法分类</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">1. 监督学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">2. 无监督学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="nav-text">2. 机器学习开发流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%B8%80%E8%88%AC%E7%9A%84%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="nav-text">3. 一般的算法工程师的工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%A1%86%E6%9E%B6"><span class="nav-text">4. 框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E7%9A%84%E5%8D%88%E9%A4%90"><span class="nav-text">5. 没有免费的午餐</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="nav-text">二、特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">1. 获取数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86"><span class="nav-text">2. 数据集划分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96"><span class="nav-text">3. 特征抽取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">4. 特征预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4"><span class="nav-text">5. 特征降维</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="nav-text">三、分类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-KNN-K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="nav-text">1. KNN&#x2F;K-近邻算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95-%E5%B8%B8%E7%94%A8%E4%B8%8E%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="nav-text">2. 朴素贝叶斯算法(常用与文本分类)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-text">3. 决策树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E9%9A%8F%E5%8D%B3%E6%A3%AE%E6%9E%97"><span class="nav-text">4. 随即森林</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E5%9B%9E%E5%BD%92%E4%B8%8E%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="nav-text">四、回归与聚类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">1. 线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-text">2. 欠拟合与过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="nav-text">3. 岭回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E2%80%93%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E4%BA%8C%E5%88%86%E7%B1%BB"><span class="nav-text">4. 分类算法–逻辑回归与二分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%88%86%E7%B1%BB%E7%9A%84%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="nav-text">5. 分类的评估方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-ROC%E6%9B%B2%E7%BA%BF%E4%B8%8EAUC%E6%8C%87%E6%A0%87%E2%80%93%E8%A1%A1%E9%87%8F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E4%B8%8B%E7%9A%84%E8%AF%84%E4%BC%B0"><span class="nav-text">6. ROC曲线与AUC指标–衡量样本不均衡下的评估</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%93K-means%E7%AE%97%E6%B3%95"><span class="nav-text">五、无监督学习–K-means算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">1. 什么是无监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-text">2. 无监督学习算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-K-means%E5%8E%9F%E7%90%86"><span class="nav-text">3. K-means原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%BC%98%E7%82%B9"><span class="nav-text">4. 优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E7%BC%BA%E7%82%B9"><span class="nav-text">5. 缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-text">6. 应用场景</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AD%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D"><span class="nav-text">六、深度学习介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-text">1. 深度学习与机器学习的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-text">2. 深度学习应用场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPU-CPU"><span class="nav-text">GPU&#x2F;CPU</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%83%E3%80%81Tensorflow%E6%A1%86%E6%9E%B6"><span class="nav-text">七、Tensorflow框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Tensorflow%E7%BB%93%E6%9E%84"><span class="nav-text">1. Tensorflow结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BC%9A%E8%AF%9D"><span class="nav-text">2. 会话</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%BC%A0%E9%87%8F"><span class="nav-text">3. 张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%8F%98%E9%87%8Fop-%E6%93%8D%E4%BD%9C"><span class="nav-text">4. 变量op(操作)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="nav-text">5. 模型保存与加载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E6%95%B0%E6%8D%AEIO"><span class="nav-text">6. 数据IO</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E4%B8%89%E7%A7%8D%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E5%88%B0Tensorflow%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%96%B9%E5%BC%8F"><span class="nav-text">1. 三种获取数据到Tensorflow程序的方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B"><span class="nav-text">2. 文件读取流程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-text">7. TensorBoard可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE"><span class="nav-text">3. 图片数据</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AB%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">八、神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Artificial-Neural-Network-ANN"><span class="nav-text">1. 人工神经网络(Artificial Neural Network,ANN)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86"><span class="nav-text">2. 神经网络原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E4%BE%8B%E5%AD%90"><span class="nav-text">3. 手写数字识别例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN"><span class="nav-text">4. 卷积神经网络(CNN)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-CNN%E5%92%8CNN%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-text">1. CNN和NN的区别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-text">2. 卷积层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E6%BF%80%E6%B4%BB%E5%B1%82"><span class="nav-text">3. 激活层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-%E6%B1%A0%E5%8C%96%E5%B1%82pooling"><span class="nav-text">4. 池化层pooling</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="nav-text">5. 全连接层</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1"><span class="nav-text">5. 网络设计</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-text">1. 一个简单的网络结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E5%85%B7%E4%BD%93%E5%8F%82%E6%95%B0"><span class="nav-text">2. 具体参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E8%B0%83%E5%8F%82%E6%8F%90%E9%AB%98%E5%87%86%E7%A1%AE%E7%8E%87"><span class="nav-text">3. 调参提高准确率</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-CNN%E5%87%A0%E7%A7%8D%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B"><span class="nav-text">4. CNN几种经典模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-MNIST%E4%BE%8B%E5%AD%90"><span class="nav-text">6. MNIST例子</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="不知自己不知"
      src="/images/head.jpeg">
  <p class="site-author-name" itemprop="name">不知自己不知</p>
  <div class="site-description" itemprop="description">以阅读为生是一件很酷的事，以写作为生是一件更酷的事。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">59</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
    <!--
    <div id="music163player">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=110 src="//music.163.com/outchain/player?type=0&id=5445280661&auto=0&height=90"></iframe>
       </div>
    -->
  </aside>
  
  
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">不知自己不知</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">412k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:14</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='255,255,255' opacity='1' zIndex='-1' count='200' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'Dv4yd2JUuvVhLBe640B6m1jw-gzGzoHsz',
      appKey     : 'sNdrMuVcw6jJzlz0HpXBNhdB',
      placeholder: "来交流交流吧~",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

    </div>
  
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"right","width":300,"height":600},"mobile":{"show":true},"react":{"opacity":1},"log":false});</script></body>
</html>



